import os
import re
import streamlit as st
from dotenv import load_dotenv

# ---- BASÄ°T Ä°MPORTLAR ----
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- .env dosyasÄ±ndaki API anahtarÄ±nÄ± yÃ¼kle ---
load_dotenv()

# --- HTML TAG TEMÄ°ZLEME FONKSÄ°YONU ---
def clean_html_tags(text):
    """HTML tag'lerini temizle"""
    clean_text = re.sub(r'<[^>]+>', '', text)
    clean_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', clean_text)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    return clean_text.strip()

# --- 1. Veri YÃ¼kleme ve ParÃ§alama ---
def get_text_chunks(text_file):
    try:
        loader = TextLoader(text_file, encoding="utf-8")
        documents = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(documents)
        return chunks
    except Exception as e:
        st.error(f"Veri yÃ¼kleme hatasÄ±: {str(e)}")
        return None

# --- 2. VektÃ¶r Deposu (FAISS) oluÅŸturma ---
def get_vector_store(text_chunks):
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)
        vector_store.save_local("faiss_index")
        return vector_store
    except Exception as e:
        st.error(f"VektÃ¶r deposu oluÅŸturma hatasÄ±: {str(e)}")
        return None

# --- 3. BASÄ°T YANIT SÄ°STEMÄ° ---
def generate_response(context, question):
    """
    LLM olmadan basit bir yanÄ±t oluÅŸturucu - DÃœZELTÄ°LMÄ°Å VERSÄ°YON
    """
    try:
        # Context'i temizle
        clean_context = clean_html_tags(context)
        
        # AnlamlÄ± iÃ§eriÄŸi bul
        lines = clean_context.split('\n')
        meaningful_lines = []
        
        for line in lines:
            line = line.strip()
            if len(line) > 20 and not line.startswith('http') and 'badge' not in line.lower():
                meaningful_lines.append(line)
        
        # Yeterli iÃ§erik yoksa tÃ¼m context'i kullan
        if len(meaningful_lines) < 2:
            meaningful_text = clean_context[:500]
        else:
            meaningful_text = ' '.join(meaningful_lines[:5])
        
        # Soruya gÃ¶re Ã¶zelleÅŸtirilmiÅŸ yanÄ±t
        question_lower = question.lower()
        
        if "tÃ¼rkÃ§e" in question_lower or "turkce" in question_lower:
            return "LangChain iÃ§in TÃ¼rkÃ§e kaynaklar ÅŸu anda sÄ±nÄ±rlÄ±dÄ±r. En gÃ¼ncel bilgiler iÃ§in resmi Ä°ngilizce dokÃ¼mantasyonu takip etmenizi Ã¶neririm. YukarÄ±daki sonuÃ§larda LangChain'in genel Ã¶zelliklerini bulabilirsiniz."
        
        elif "nedir" in question_lower:
            return f"LangChain: {meaningful_text[:300]}... [Detaylar iÃ§in yukarÄ±daki sonuÃ§lara bakÄ±n]"
        
        elif "nasÄ±l" in question_lower or "kurul" in question_lower:
            return f"Kurulum: {meaningful_text[:400]}... [AdÄ±mlar iÃ§in detaylÄ± sonuÃ§larÄ± inceleyin]"
        
        elif "ne iÅŸe yarar" in question_lower:
            return f"KullanÄ±m: {meaningful_text[:350]}... [KullanÄ±m alanlarÄ± iÃ§in sonuÃ§lara bakÄ±n]"
        
        else:
            return f"{meaningful_text[:500]}... [DetaylÄ± bilgi iÃ§in yukarÄ±daki sonuÃ§larÄ± okuyun]"
            
    except Exception as e:
        return f"Ã–zet: {clean_html_tags(context)[:400]}... [Detaylar iÃ§in aÅŸaÄŸÄ±daki sonuÃ§lara bakÄ±n]"

# --- 4. KullanÄ±cÄ± Sorusunu YanÄ±tlama ---
def user_input(user_question):
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        if not os.path.exists("faiss_index"):
            st.error("FAISS index bulunamadÄ±. LÃ¼tfen Ã¶nce index oluÅŸturun.")
            return
            
        new_db = FAISS.load_local("faiss_index", embeddings)
        docs = new_db.similarity_search(user_question, k=3)
        
        st.success(f"âœ… '{user_question}' sorusu iÃ§in {len(docs)} sonuÃ§ bulundu:")
        
        # TÃ¼m dokÃ¼manlarÄ± birleÅŸtir
        all_context = "\n".join([doc.page_content for doc in docs])
        
        # YanÄ±t oluÅŸtur
        with st.spinner("YanÄ±t oluÅŸturuluyor..."):
            response = generate_response(all_context, user_question)
            
            st.info("ğŸ’¡ **YanÄ±t:**")
            st.write(response)
        
        # DetaylÄ± sonuÃ§lar
        st.divider()
        st.subheader("ğŸ“„ DetaylÄ± SonuÃ§lar")
        
        for i, doc in enumerate(docs):
            clean_content = clean_html_tags(doc.page_content)
            if len(clean_content) > 50:
                with st.expander(f"ğŸ“– SonuÃ§ {i+1}"):
                    st.write(clean_content)
        
    except Exception as e:
        st.error(f"Hata: {str(e)}")
        st.info("LÃ¼tfen sayfayÄ± yenileyip tekrar deneyin.")

# --- STREAMLIT ARAYÃœZÃœ ---
st.set_page_config(
    page_title="Akbank GenAI Chatbot",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.header("ğŸ¤– Akbank GenAI Bootcamp Projesi: RAG Chatbot")

# Session state
if 'current_question' not in st.session_state:
    st.session_state.current_question = ""

# Sidebar
with st.sidebar:
    st.subheader("ğŸ“‹ Proje Bilgisi")
    st.write("RAG (Retrieval Augmented Generation) tabanlÄ± chatbot")
    st.write("**Veri:** LangChain dokÃ¼mantasyonu")
    
    st.divider()
    
    st.subheader("âš™ï¸ Sistem")
    
    if st.button("ğŸ”„ Index'i Temizle ve Yeniden OluÅŸtur", use_container_width=True):
        if os.path.exists("faiss_index"):
            import shutil
            try:
                shutil.rmtree("faiss_index")
                st.success("âœ… Eski index temizlendi!")
            except Exception as e:
                st.error(f"Silme hatasÄ±: {str(e)}")
        
        with st.spinner("Temiz index oluÅŸturuluyor..."):
            try:
                text_chunks = get_text_chunks("data.txt")
                if text_chunks:
                    vector_store = get_vector_store(text_chunks)
                    if vector_store:
                        st.success("âœ… Temiz index oluÅŸturuldu!")
                    else:
                        st.error("âŒ Index oluÅŸturulamadÄ±!")
                else:
                    st.error("âŒ Veri yÃ¼klenemedi!")
            except Exception as e:
                st.error(f"Hata: {str(e)}")

# Ana iÃ§erik
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ’¬ Sohbet")
    
    if os.path.exists("faiss_index"):
        st.success("âœ… Sistem hazÄ±r! SorularÄ±nÄ±zÄ± sorabilirsiniz.")
    else:
        st.warning("âš ï¸ LÃ¼tfen Ã¶nce sidebar'dan index oluÅŸturun!")
    
    st.divider()
    
    user_question = st.text_input(
        "LangChain hakkÄ±nda sorunuzu girin:",
        placeholder="Ã–rnek: LangChain nedir? NasÄ±l kullanÄ±lÄ±r?",
        key="question_input"
    )
    
    if st.session_state.current_question:
        user_question = st.session_state.current_question
        st.session_state.current_question = ""
    
    if user_question:
        user_input(user_question)

with col2:
    st.subheader("ğŸš€ Ã–rnek Sorular")
    
    examples = [
        "LangChain nedir?",
        "LangChain nasÄ±l kurulur?",
        "LangChain ne iÅŸe yarar?",
        "LangChain'in temel bileÅŸenleri nelerdir?",
        "RAG (Retrieval Augmented Generation) nedir?",
        "LangChain ile neler yapÄ±labilir?"
    ]
    
    for example in examples:
        if st.button(example, key=example, use_container_width=True):
            st.session_state.current_question = example
            st.rerun()

# Footer
st.divider()
st.caption("Akbank GenAI Bootcamp - RAG Chatbot | TemizlenmiÅŸ Ä°Ã§erik")